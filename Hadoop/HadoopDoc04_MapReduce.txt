

Processing data with mapreduce ____________________________________________________________________

We will set up a simple mapreduce task. Count the word frequecy in the large text file.


A distributed system involes processes which runs on multiple machines, which are interconnected to form something called cluster. 
Mapreduce is the programming paradigm which allows processing on the distributed compuing system.

Dataset is itself distributed on multiple machines. Mapreduce reduce helps us in how do we run processes on These multiple machines and bring these together to achieve a common abjective.

Data sets are petabytes in size.
Each file is distributed across many machines in a cluster. Each machine holds a partition of data.

Mapreduce breaks up the processing of task at this scale into two distinct phases. Map(runs on multiple node in a cluster) and second pahse reduce(takes the output of the map phase and give us the final result).

Map processes work on the data that lives on their own machines(data nodes). What ever the output the map phase generates is then collated together, trnasfered across the network within the cluster to ONE node TYPICALLY, where the reduce phase runs. Now this reduce phase can also run on multiple nodes.


As a developer, we just have to right the logic for just map and reduce, and everything else, like fault tollerence, replication, distributed processing is taken care behind the scene by the hadoop framework.

Map phase runs on one subset of data, so there are several map processes which run to processes the entire dataset. The code needs to written for the map phase should process only one record.
Input to the code is one record.
Figure out the output for this record, it must be in a form of a key value pair.

Mapreduce only deals only in terms of key value pairs.


Reduse pairs collated all the key value pairs that it receives from the map phase, and figures out how to combine the results in a meaningful way. It takes map output, finds all the values that are associated with the same key, and then combine these values in some way.

Map is the step performed in paralled and reduce step to combine the intermediate results.

A large file might have distributed across multiple nodes, and each subset of a file on a machine is calles a partition. [I believe a partition of a machine can consists of all the block of that file on that machine]

A file is distributed across multiple machines, and a map processes is run on each of these machines and each map process processes the input data that is present on that machine. All the MAPPERS run in paralled.
Q. Again the question is whether a process for each block or for each partition/node??***


Within each Mapper, the records that are available are processed one at a time, one records at a time.

Take a record, and the output of a map phase emits a key-value pair. This output is passed to the reducer process.

Typical that the reducer performs is to combine all values which have the same key. 

Writting a mapreduce program boild down to answering two key questions
1. What key value pair should be emitted in map phase, so that the reduce phase can use it for final result?
2. How should the values with the same key be combined?

Output of Map and Reduce phase depends on our objectives. Say if we want a work frequency, then output of the map for a songle word is {twninkle, 1} ({key, value})

Output of all the mappers are passed on to a Reducer process.
Typical job of a reducer is to combine all the key's with some logic. Output -> {twninkle, 4}



Implementation in JAVA __________________________

Code to mapreduce is typically implemented in java.
Any mapreduce job comprises of three basic components
1. Map class, where we implement the map process.
2. Reduce class, has logic where the combining logic on the key-value pairs.
3. Main class, which is the driver program which kick-starts your mapreduce, configure it, and set it to run.

The map logic run by the mapper processes is typically implement in a single class, which may of course use other utility classes to process data.

The map	class extends the base mapper class in hadoop library. Mapper class has whole bunch of helper methods, we just need to override only one method in order to plugin our code, this method name is MAP

Map class is the generic class with 4 type parameters. 
<input key type,
<input value type,
<output key type,
<output value type>

The type parameters specify the data type of the input keys and value that are fed into the map phase, the output keys and value that are output from the map phase.



Similarly, reduce class.
extends Reducer class
method to override : reduce method

<input key type,
<input value type,
<output key type,
<output value type>



The entire concept of the mapreduce is that the output of the mapper is processed and fed as the input to the reducer. So output data types of the mappeer has to matched with the input data type of the reduce phase.



Main Class:
It is the driver program.

Mapper and the reducer classes are used by the job that is configured in the Main class.

It sets up the job instance, which contains the configuration parameters of the Mapreduce and serves as the entry point of our mapreduce job.
The main class instantiate the job object, which has the configuration parameters, and it is this job object that's submitted to the hadoop cluster in order to get the mapreduce running.

Job object has a whole bunch of properties that need to be configured to run mapreduce successfully, where the input files are, write should the output parts be stored, what are the key-value data types that the mapper and reducer use, where does the logic of the actual Mapreduce that has to run. All of this is in the job object. 





Implementing Code:

Create a new project and add the following jars from the hadoop distribution directory:
share/commom
share/hdfs
share/mapreduce
share/yarn
share/commom/lib



Map Class:

package wordcount;

import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Mapper;

import java.io.IOException;

public class Map extends Mapper<LongWritable, Text, Text, IntWritable>{

    public void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException{

        String line = value.toString();
        String[] words = line.split(" ");

        for(String word :words){
            context.write(new Text(word), new IntWritable(1));
        }
    }
}



Map Class reads on record of the file from a file each time. One records is a one line of a text.
Context in above code, is the bridge to the outside world from within our map-reduce code.
Context talks with the map-reduce framweork, and the way we pass the result of the map phase to pass it to the reduce phase is by writting it to this Context


Reducer Class:

All the key-value pairs from the output of the Map Phase is collected by the framework and passed to the reduce function, that's we there is a itterable type in reduce function.
Ex, {key, valueS<iterable_containing_all_values>}

Once logic is implememted, we pass it down to the context and the context will pass it down to the map-reduce framework which will write it down in HDFS.


Main Class:
Instantiate a Job class which holds the configuration parameter of the job.






To run code on hadoop cluster, we need to compile the java code to JAR(java archive). Build a jar for this project. 
in IntelliJ, File->Project Structure -> Artifacts
generate the jar for the current mapreduce code. 
make a director on the HDFS for to store the input files, let it be in dir /mapreduce/input  (this mapreduce dosent meant anything)
$ hadoop fs -mkdir /mapreduce
$ hadoop fs -mkdir /mapreduce/input

copy some files into this input directory to test out mapreduce program.
$ hadoop fs -copyFromLocal  ../etc/hadoop/*  /mapreduce/input


Now we submit out mapreduce program to the cluster, this we do as follow:
#hadoop jar ../MapReduceJars/HadoopWordCount.jar  wordcount.Main /mapreduce/input /mapreduce/output --this is not working as wordcount.Main is passed into as the inputFile name
#we have removed it, then it is working, but now how does hadoop knows about the wordcount.Main now???***


hadoop jar ../MapReduceJars/HadoopWordCount.jar wordcount.Main  /mapreduce/input /mapreduce/output
wordcount.Main -> Is the main class of our Map-Reduce program, which contains the main() method. Driver Program. Shoudl be full path to the class. As it is in wordcount package, so wordcount.Main

-> Output dir should be a non existent directory


-- MacOS is set to case-insensitive by default and this cause some issues with the META-INF/LICENSE file that IntelliJ adds to the JAR and hadoop jar command doesn't really play well with it.
-- To fix is to simply delete this file from inside the JAR
zip -d HadoopWordCount.jar META-INF/LICENSE



Monitering:
Resource Manager-> localhost:8088
localhost:8088/cluster


***why even we have just a single node, there are multiple MapReduce jobs are trigerred?
As we have seen in the monitering, there are 30 mapreduce job trigerred for the wordcount example. Block size in the hdfs is 128 MB.
One intution is that, as the input dir contating 30 files, thus one mapreduce job is trigerred for each file. 
Also each file can be in different block in same node in hdfs.

