Offline Coruse stored at:
C:\Users\Notebook\AppData\Local\Pluralsight
D:\SoftwareInstalled\PluralsightApp\CourseData


Hadoop is built in JAVA. So Java installation is required to run hadoop. 


Hadoop contains HDFS as the storage system. MapReduce as the proramming Model for parallel processing. YARN as the Resource Negoriator.

Hadoop < 2.0 (HDFS + MapReduce)
Hadoop >= 2.0 (HDFS + MapReduce + YARN)



Some statistics:
Facebook : 	Stores 300 PB of data [300*1024 TB]
			Processes 600 TB of data a day
			User per month 1 billion
			2.7 billion per dat
			300 Million photos as day

NSA : 		Storage 5 Exabyte (approx)
			Process 30 PB per day

Google : 	Sotres 15 Exabyte
			Process 100 PB per day
			Indexed 60 trillion indexes
			Search per second 2.3 million
			Unique search user > 1 billion users per month 


Big Data Means: (Distributed Computing Framework)
	Stores Raw Data
	Process it in timely manner
	Scale :- Accomodating changing needs, Infra should be flexible to change, expand.



Two ways to build a system:
Monolithic: 	Everything on a single machine as a one big chunk
				A single porweful machine
				Performance of a single machine doesn't scale linearly with cost

Distributed:	Multiple machine and multiple processes which run on these machines
				A cluster of decent machines that know how to parallelize
				Cluster of machine can scale linearly.
				That is, if the no Nodes are doubles, then Storage and Speed both gets almost doubles.


With multiple machines, they need to be co-ordinated so to work and achive a single goal.
A software will perform this co-ordiantion.  
Cordination task include:
	Partitioning the data with replication to cope up with faliure
	Co-ordinaing computing tasks on multiple machine
	Fault tolerance and recovery 
	Sortware needs to ensure that a particular node has the resource required for the process to run. Memory, Compute capacity, Hard-Disk space.
With this sortware, Developers would be abstracted away with the details of the managing all the above tasks.


Objectives of this software:
	To stores millions of records on multiple machines, and keeping track of what record exists on which node.
	To run processes across these machines and co-ordinate in such a way to achive a single object. <- Most IMP
	Fault tollerance and Data recovery
Goolge initially comes up with GFS (Google File System) and MapReduce
	GFS: To solve Distributed Storage. A file system not on a single machine but on multiple machine which made up a cluster.
	MapReduce: To solve the Distributed computing running process in parallel and briging the results of these processes together.  
Equavelnt to GFS in Open Source -> HDFS, and MapReduce -> MapReduce ()
HDFS + MapReduce -> Hadoop

HDFS -> A file system to managing the storage of data across multiple machines
MapReduce -> A Framework to processes data across multiple servers 



2013 -> Apache Hadoop 2.0 
	MapReduce was splits into two separate logical parts. MapReduce and YARN
	Responsibility made more narrower and focuesd.
	MapReduce -> A Framework to define the data processing task
	YARN -> A Framework to run the data processing task across multiple machines. Manage:-resources, Memory, Storage
	YARM do not care what the data processing task is.  
	YARM is only responsible to running it ans seeing it through to completion.

Each of HDFS, MapReduce and YARN has their configuration file.


Mapreduce_2 defines what computation you want to perform on the data. User define Map and Reduce tasks using MapReduce API (available in various programming languages)

Lifecycle of a MapReduce Job:
MapReduce programs are packages into Jobs. Hadoop deals with jobs as a unit entiry.
Jobs are triggered on the cluster using YARN. 
YARN check that the cluster and the nodes on the cluster have the resources in order to run this Job.
YARN figures out where and how to run the job and stores the result in HDFS.


To not to restrict the Hadoop to just core Java Developers a whole ecosystem as evolved around it:
HIVE  -> SQL interface to Hadoop  (Bridge to Hadoop for those who don't have exposure to OOP in Java). Convert SQL like queries to MapReduce behind the scene.
HBase -> A database build on HDFS to allow low-latency operation on Key-Value pairs. Data in HBase can be looked up and scanned very quickly. 
Pig   -> A scripting language. A data manipulaton language. Allows data to take in a very raw form, example log data. A way to convert unstructured data into structured form.
         This structured format can then be stored in HBase and Hive and then can be queried.
Oozie -> A workflow management system. A tool to schedule workflows on all Hadoop ecosystem technologies.
Flume -> Allow to store and get data in/out of Hadoop
Sqoop -> Allow to store and get data in/out of Hadoop
Spark -> Transformation in functional way. A distributed computing engine. Interactive shell.


