https://www.youtube.com/user/intellipaaat/search?query=flume

Flume Introduction:
Cloudera Flume -> Apache Open Source later

Flume is Distributed and Reliable.
It is effcient tool for collecting, Aggregator and Moving data (not only log data/files) from many different sources to a centralized location.

Robust, Fault Tolerant (What about memory Channel **??), and many failover and recovery mechanism.



Initial Problem Statement:
There are lots of servers, Web-Servers, Application Servers, which are generating log data.
[Now, since the data sources are customizable, data can be transformed from social-media, email, network teaffic data, and pretty much every data source is possible]



By analysing the logs we can get a log of information. Like Customer insight, Server faliuere analysis and predictions, Server capacity utilization, website hits etc.
So there logs are very useful for analysis.

We want this huge data into Hadoop to analyse it.

Application server and Hadoop Cluster can be in different geographically location. 

How to bring these log file in Hadoop?
Sqoop cannot be used as this data is not structured data, its just plain log files.


Data Collections is Ad-hoc ()
Getting Data in Hadoop
Streaming Data

Initially Flume was created to copy log files in Hadoop. Later Streaming Data.



Flume agent can be setup which keeps on brining the Streaming the data.
Flume Agent is the process which will bring tha data. We need to configure the agent.
Multiple Flume agents can be configured.
A single Flume Agent can connect to multiple producers, or each agent can only connected to a particular producers. (Say, if data volume is HIGH)


Many different data flows are possible:-
	One Flume Agent for all the work
	Each individual separate Flume Agent for each flow.
	Multiple Flume agent to a single Flume agent, which then writes it down to the HDFS.
	

Flume can run anywhere. Not necesssarity on Hadoop. 



Flume Architecture ________________________________________________________________________________

Flume consists of Flume Events(Byte Payload), Flume Agent [Source, Channel and Sink]

Flume Agent :-
	Is a JVM process that hosts the components throuch which events flow from an external source to next destination. 
	It comprises of Source, Channel(s) and Sink(s)
	Q. Can a single flume agent have multiple sources too? ***


Flume Event:
	This is the basic unit of data transported inside Flume.
	[Byte Payload + Optional set of string attributes]


Source:-
	Event flow from data generators to Flume Source.

	A Flume source consumes events delivered to it by an external source, like a web server.
	The external source deliveres events to Flume in a format that is recognized by the target Flume source.
	For example, an Avro Flume source can be used to receive Avro events from Avro clients or other Flume agents in the flow that send events from an Avro sink



	Diferent Type of Sources: (Each source receives events from a specified data generators)
		Avro Sources
		Thrift Sources
		Twitter


Channel:-
	When a Flume Source receives an Event, it stores it into one or more Channels.
	The Channel is a passive store that holds the Event until that Event is consumed by a Sink.
	It buffers the data till it is consumed by the Sinks.

	A Channel can work with any numeber of Sources and Sinks.

	Diferent type of Channels:
		JDBC
		File System Channel:
			Backed by Local file system.
		Memory Channel:
			Stored events in Memory, which is faster but any events still left in Memory Channel when an agent process dies, can't be recovered.
			If source is UDP, some negligible data loss should be accepted.


Sink:-
	A Sink is responsible for removing an Event from the Channel and putting it into an external repository like HDFS, OR forward it to next Flume Agent.
	Sink sotres the data in the Centralized location.
	It consumes events from Channel and deliever it to the destination
	Destination might be another Agent or a Centralized store

	Diferent type of Sink:
		HDFS Sink
		Hive 
		Hbase




The Source and Sink within the given agent run asynchronously with the Events staged in the Channel.



MultiAgent Flow:-
	If multiple flume agents are linked, then the Sink of the previous agent and the Source of the current agent needs to be of same type 
	Example both needs to be of Avro type if you are using Avro data to be passed.


A single Source can send event to multiple Channel, and only one sink can be attached to each Channel.




Multiplexing vs Replication _______________________________________________________________________

Replication:
	Each event is sent to all Channels

Multiplexing:
	A Source can specify multiple Channels but a Sink can only listen from a single Channel.

	Each event is sent to a subset of available Channels based on if the events value matches a pre-configured value.
	These mappings can be set into the agents configuration file.





Flume Interceptor _________________________________________________________________________________

Interceptor can modify/transform/drop events in-flight based on a criteria.
Transformation available before dumping event into Sink (At which stage does this transformation happen, during dumping or somewhere in the Channel?? ***)


Flume supports chaining of Interceptors.





Multiple Flume agent can be combined with one another to form layers, with sink of one Agent is sending even data to source of second agent.
In such situation, the type of data should be consistent between the Sink and Source. That is the event data fromat output by the sink should match 
with the event data Source type of the next layer Source.


Q. I think there can be multiple target system are also possible, like not just a single table, and not a single type of target system. ***


