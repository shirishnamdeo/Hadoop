Introduction 

Hbase is distributed database, scalable, allows real-time processing.
HBase database is built on Hadoop Distributing Computing Framework. 
HBase has ability to deal with billions of rows of data, where each record can contain millions of fields. 
HBase allowis real time random access.
Optimize disk seeks as reduce read laency.


Hadoop is a distributed Computing Framework. It is not a Database.
A database goes beyond just storing and processgin of data and provided many other guarantees to the user. Hadoop does not.


RDMBS has a very specific structured layout of their data. All data is stored within rows and columns and therse rows and columns make up a table which contrains one logical unit of data.
Random Access -> Organizations want to update one row at a time. RDBMS allows random access to a single row of the data. 
Thee db are very fast read/write/update.
RDBMS as ACID compliant (Atomocity, Consistency, Isolation, Durability)


Atomicity:
Transactions on a database should be all-or-nothing. They should not be partially applied. Ex. Transfer of money.

Consistency:
Database updates should not violate any constraints applied on the data.
Like Not NULL, UNIQUE, primary-key, foreign-key.  

Isolation:
Concurrent operations on the database should appear as though they were applied in some SEQUENCE.
Isolation also determines the granularity of the updated on the data. Which bits of data we can update at the same time?

Durability:
Once changes have been made to the data, they are permanent.





Limitations of Hadoop:

Hadoop in its bare bone form makes a very poor database.
Data stored in hadoop is UNSTRUCTURED. Hadoop has not idea of what is within those files. Data in hdfs has no schema, no rows, columns. They are simply text-files. (Text files, log files, Audio, Video files). HDFS doesn't care what is within those files.
For files, say CSV, XML, JSON, Here we as a developer has to impose a structire upon the data. Hadoop has no feature of checking whether the files are well formed, structures, whether all the fields are there or not.

Hadoop does not allow to access a single or a bunch of records at random. 
Cannot create, access and modify individual records in a file.
MapReduce parses ENTIRE files to extract infromation.

Hadoop is not suited for real-time processing. Hadoop -> Batch processing.
Time required to seeking a particular record or a field is very HIGH.

Hadoop is NOT ACID compliant. No guarantees for data integrity. 


So Google needs some kind of database which runs atop of distributed computing environment. Database for extremly fast lookup in real time (Search results)

Google BigTable -> A distributed storage system for STRUCTURED data where individual rows can be accessed quickly and modified quickly. 
Google BigTable -> HBase distributed database (Open Source)

Hbase is distributed database, as the data stored is in the form of files on HDFS.
Hbase abstracts this file representation, and provide the data in from of records, which is indexed by Rowkey.

Hbase is reliable -> Fault tollerance and recovery features of HDFS.
Data backup is by replication.
Metadata backup is b ising a secondary NameNode/Or backing up metadata files on remote disk.


Hbase is aware of the structure of data stored within it. Though the structure is very loosely defined. 
No specifications on how many columns and what kind of data needs to be stored in those columns.

Hbase has a way to index every record that is stored within it using -> Rowkey
Rowkey allows access to individual or series of records.
Hbase also allows random access to records using Rowkey index.
HBase does not follow ACID compliance as stricly as RDBMS, however all transactions which update the columns in a single row will be ACID compliant, but across multiple rows will not be.

Hbase alos allows batch processing using map-reduce. -> Best of both worrld.



Hbase vs Releational Database

Hbase:  Columunar storage
		--One row of the RDBMS row will be multiple in HBase Columunar storage.
		--A Columunar db store conver every row of data into multiple rows along with its unique identifier,  the unique identifier is now called the Rowkey. 
		--They Rowkey is unique for every record of the columunar store and one record has as many logical rows as there are attributes in that record. 
		--The number of rows in a columunar sotre that one records translate to depends on the number of attributed the particular record have.

		Advantage of Columunar Store:
		-- Sparse Tables (Where every record do not have value for each of the column): 
			No wastage of space when storing sparse data.
			Empty cells in RDBMS still occupy space!!
		--Dynamically add attributes to every record.
			Doing so in traditional database would require a schema change. 
			

Hbase: Denormalized form
		-- Traditional databases use normalized forms fo database design to minimize redundancy (so that same column should not be stored in multiple table, as it makes updates difficult and prone to integrety errros, out of sync).
		-- Also in normalized from, data is stored in multiple tables so that each table holds a logical unit or semantic unit.
		-- Normalization help in minimize redundance, makes updates simple and ensures that the data within the database remains in sync.
		-- Normalization -> Optimize the storage. 
		-- In distributed system, storage is cheap. And whant we want to optimize is NO of the DISK SEEKS.
		-- Locate the data the time consuming thing in distributed system.
		-- This is achieved by De-Normalizing the storage.

		-- HBase allows complex datatypes within a single cell, like Arrays, Struct
		-- Now all that we need to extract all the information for a Rowkey is a single read operaion to get the entire record. The record contains everything. 
		-- One disk seek will give us all the information. 


HBase: CRUD only operations
		-- Traditional Database has a RICH SQL language, which has rich functionality for retrieve infromaion from a database. Like Joins can combine data from multiple tables. 
		Or Group Data and Aggregate data on the column level. Sort Data. 
		--This rich functionality is not available in HBase.
		-- Hbase does not support SQL -> NO SQL  

		-- HBase only allows Create, Read, Update and Delete operations.
		-- Hbase only supports sinlge row, cell operations 
		-- One important reason that data is stored in de-normalized from in HBase is because queires which span multiple tables are simple not allowed. 
		-- No JOINS, INDEXDEX (only just Rowkey is present), No contains to check the validity of a field, like NULL, UNIQUE, foreign-key.  
		-- Due to this, all the details pertaining to a sinle entity is self-contained in one wor that represents that entity. 


HBase: ACID at the row level
		-- Hbase is ACID compliant at the row level, not across the database level or multiple tables.
		-- Updated to all the columns in a one single row is atomic. Either all column you have specified for a row will be updated or none. 
		-- Updates to multiple rows will not be atomic, even if the same column of those rows. 




Installation of Hbase on Pseudo Distributed Mode:
--For Hbase to work in pseudo-distributed mode, Hadoop must also be deployed in pseudo-distributed mode as well.

hbase-site.xml:-
<configuration>
	<property>
		<name>hbase.cluster.distributed</name>  -- To indicate that HBase runs in pseudo-distributed mode.
		<value>true</value>
	</property>
	<property>
		<name>hbase.rootdir</name>  
		<value>hdfs://localhost:9000/hbase</value>
	</property>
	<property>
		<name>hbase.zookeeper.property.dataDir</name>
		<value>file:///D:/SoftwareInstalled/Hbase/hbase-2.1.1/zookeeper_storage</value>
	</property>
</configuration> 

Hbase store data in hdfs. 
hbase.rootdir -> Points to the location on HDFS. 
Make sure the hbase folder on hDFS doesn't exists if you are running a fresh installation. 
If the hbase folder already exists in hdfs, then hbase will try to migrate the data.

Hbase relies on a service called ZooKeeper. ZooKeeper is a service which acts as a co-ordinator or a manger in distributed systems. 
ZooKeeper is responsible for configuration management, synchronization and a whole bunch of stuff in a distributed Computing environment.
So make sure zookeeper has a place where it can store data.  



Next step is tell HBASE where the JAVA SDK are installed. 
hbase-env.cmd [I believe JAVA confguration is not needed anymore, as I can't see a variable in this file now.] Any way just set it there
set JAVA_HOME=D:\SoftwareInstalled\Java\JDK8


Set the PATH variables

HBASE_HOME
D:\SoftwareInstalled\Hbase\hbase-2.1.1

HBASE_BIN
D:\SoftwareInstalled\Hbase\hbase-2.1.1\bin


-> Start Hadoop
start-hbase -> Not running in Windows.
ERROR-> This is not implemented yet. Stay tuned.

Trying in Ubuntu (Windows SubSyetem):-
wget -> to download
tar -xvf tar-file

export JAVA_HOME=/usr/lib/jvm/java-8-oracle -> in hbase-env.sh

export HBASE_HOME=/home/notebook/hbase/hbase-2.1.1
export HBASE_BIN=/home/notebook/hbase/hbase-2.1.1/bin

export PATH=$PATH:$HBASE_BIN

Also need to set up password less autherntication.
If even after you keep getting this error 'notebook@localhost: Permission denied (publickey).'
Set it up suing sudo on root account and run hbase on root account too.

ssh-keygen -t dsa -P '' -f ~/.ssh/id_dsa
cat ~/.ssh/id_dsa.pub >>  ~/.ssh/authorized_keys

Note: if we create an rsa keys, and append them into the authorized_keys files then ssh will work with passwordLess authentication, without any configuration changes.
	What about the dsa keys. Right now not working with dsa keys directly. 
	
	solution to this is : add this line to /etc/ssh/sshd_config file 
	PubkeyAcceptedKeyTypes=+ssh-dss 
	and same line to a 
	~/.ssh/config file. adding into both is mandatory.

-> ssh localhost

-> service ssh restart 
-> start-hbase.sh

--Somehow master server stops in some time. Need to kill zookeeper and Region server before restarting
./stop-hbase.sh doesnot kill Regionserver if master is not present.



If connection refused error: (we might need to configure passwork less authentication)
sudo apt-get install openssh-client openssh-server







Once hbase start running, make sure HBase processes are up.
HRegionServer -> Data Stored in HBASE is divided into reqions and every region has a region server.
HMaster       -> HMaster service is a master service which manages all the region servers.
HQuorumPeer   -> ZooKeeper Coordinatig Software. 


Hbase UI : localhost:16010
http://localhost:16010/master-status





Executing CURD operations using Hbase-Shell: ______________________________________________________

Traditional Database has 2 dimensional Data Model, where a primary key and a column uniquely identify a cell.

Hbase has a 4 dimensional data model.  
RowKey:
	-- A unique identifier for any row/record. It is very similar to a primary key in a Traditional Database.
	-- A table can have any number of column families.
	-- Hbase builds an index based on the Rowkey. 
	-- It is very fast to look-up individual records or a range of records using Rowkeys.
	-- Rowkey can be any data structure, can be primitives(INT/LONG/STRING...), structures(structs), arrays.
	-- Rowkey is internally represented in Hbase as byte-arrays, that is why complex data types are also supported.
	-- Rowkeys when stored in HBase are sorted in ascendin order.
	-- Hbase partition the data that it stored within a table into subsets called REGIONS. And these subsets are basically a range of Rowkeys. Ex, Rowkeys (1-100) in one region, (101-200) in another region and so on.


Column Family:
	-- Every record has a column family and column families are logical units fo data that are stored within a single record.
	-- A column family is a group of columns which are releted to each other in some way. They for a logical grouping.
	-- Such as all fields of an address can be a part of a single column family
	-- Column family in any table is specified when the table is created and its schema is initialized.
	-- Column family are defined at schema definition tme and once they are setup the are hard to change.
	-- Changing it might involve data migration (depends on the change we are making). 
	-- A column family can have any number of columns, and we don't need to specify this upfront at the time of declaring the table.
	-- It is also not necessary that every column of a column family have a value associative with it. We can have column in a column-family which are empty.
	-- Only the larger logical units, the column family, has to be the part of table schema. Column can be defined on the fly.

	-- Every table is divided into groups of data and one group is represented by a column family.
	-- All records within a family have the same set of column families.  
	-- Hbase internally, lays out the data such that every column family is stored in a separate file.
	-- When data is queried in Hbase, all column of a family are fetched together.

	-- While creating a table, ensures that you have atleast one column family. Every Hbase table needs atleast one column-family.


Column (Qualifiers):
	-- Columns are the unites where actual data is stored.
	-- No SCHEMA and NO DATA-TYPE accociated for a column. Everthing internally is just a byte array. This makes storage very flexible.
	-- Columns are not the part of schema definition, and can be dynamically added.
	-- Each row can have different column specified for the column family.
	-- Ecah column must belong to a column-family.
	-- Column is identified by prefixing the name of the column-family with a colon before the column name
	-- column-family:column-name


Timestamp:
	-- The values contained within a column for a single recors have a version (Timestamp) accociative with it. This verison is the timestamp at which the value is edited or modified.
	So if you have update a value of a column multiple times, all versions of the value are preserver along with the associated timestamp.
	--When we query data for a column, the version with the latest timestamp is returned to user.
	-- Timestamp is the epoch time, since the January 1, 1970

These for dimensions together, form a ROW in HBase table.





Hbase Shell Commands: ___________________________

status
whoami -> What user you are currently logged in as.
list -> list of tables
list_namespaces ->



create 'table-name', 'column-family1', 'column-family2'

describe 'table-name' -> To see how a particular table looks like.

count 'table-name' -> No of rows in a table.

put 'table-name', rowkey, 'column-family:column', 'value'
put 'table-name', rowkey, 'column-family2:column2', 'value2'
First argument after the table-name is the Rowkey. Rowkeys cab be any data-type, int, string, structure.
Put command added an NEW value if it doesn't exists or UPDATE and existing value

scan 'table-name'
-> scan will retrieve all the records from the given table.
 
A precise value of a CELL is the timestamp and the value added at that timestamp for any column is a CELL.

On updated, values with the old timestamp will still be present but SCAN won't retrieve that value.

HBase internally sotrs all the records and aranges them in ascending order of Rowkeys.



get -> Command allows to retrieve the data of a single row.
get 'table-name', rowkey
-> Hbase will random access that Rowkeys, and retrive all the columns of that Rowkey.

get 'table-name', rowkey, 'column-family:column'
get 'table-name', rowkey, 'column-family:column', 'column-family:column2'
-> Get can retrieve data of a particular column for a particular rowkey key too.


-> Scan also takes a dictionary. 
scan 'table-name', {COLUMNS => ['column-family:column']}
scan 'table-name', {COLUMNS => ['column-family:column'], LIMIT => 1}
scan 'table-name', {COLUMNS => ['column-family:column'], LIMIT => 1, STARTROW => rowkey}  ->Start from the Rowkeys 'rowkey'


scan 'table-name', {COLUMNS => ['column-family:column'], STARTROW => 3, STOPROW => 10}  -> STOPROW excluded



Delete command allowd individual cell from a Rowkeys

delete 'table-name', rowkey, 'column-family:column'


Once a table exists in HBase and is enabled, it keeps two pieces of information in Memory:
-- Index of all the row id's, so that it can quickly loopup records by RowKey
-- Log of recent changes

To drop a table, we need to first disable it. HBase removes the index from memory and flushes the recent changes to disk. 

disbale 'table-name'
enable 'table-name'
exists 'table-name'
drop 'table-name'
truncate 'table-name'





HBase JAVA API: _________________________________


A configuration Java object to hold all the configuration.

All table releted operations are performed with the Administration object.
To manipulate the data in table, we will use the table instance to preform the operation, eg put, get, scan.


Hbase stores all data internally as byte arrays, so while inserting data in Hbase, we need to conver them to Byte array. Even the name of the columsn, column family, data, etc. all have to be byte arrays
Row Id also has to be specified as a byte array



Filters: ________________________________________

Filter allow to control what data is retrned from a scan operation.
Scan has built-in filters



Integrating MapReduce with HBase: _______________

MapReduce can also help with complex commands which are not possible with HBase shell too.

SQL allows operations like Grouping, Sorting, Aggregation, JOINs multiple tables. HBase doesn't provide these operations. 
There is a way to programmatically manipulate the data that lives in HBase tables, with MapReduce.

Key-Value of the Map-Input can be of any data types.
